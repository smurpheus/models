default_configs = [{'type': 'integer', 'name': 'num_clones', 'value': 1, 'description': 'Number of clones to deploy.'},
           {'type': 'boolean', 'name': 'clone_on_cpu', 'value': False, 'description': 'Use CPUs to deploy clones.'},
           {'type': 'integer', 'name': 'num_replicas', 'value': 1, 'description': 'Number of worker replicas.'},
           {'type': 'integer', 'name': 'startup_delay_steps', 'value': 15,
            'description': 'Number of training steps between replicas startup.'},
           {'type': 'integer', 'name': 'num_ps_tasks', 'value': 0,
            'description': 'The number of parameter servers. If the value is 0, '
                           'then the parameters are handled locally by the worker.'},
           {'type': 'string', 'name': 'master', 'value': '', 'description': 'BNS name of the tensorflow server'},
           {'type': 'integer', 'name': 'task', 'value': 0, 'description': 'The task ID.'},
           {'type': 'string', 'name': 'train_logdir', 'value': None,
            'description': 'Where the checkpoint and logs are stored.'},
           {'type': 'integer', 'name': 'log_steps', 'value': 10,
            'description': 'Display logging information at every log_steps.'},
           {'type': 'integer', 'name': 'save_interval_secs', 'value': 1200,
            'description': 'How often, in seconds, we save the model to disk.'},
           {'type': 'integer', 'name': 'save_summaries_secs', 'value': 600,
            'description': 'How often, in seconds, we compute the summaries.'},
           {'type': 'boolean', 'name': 'save_summaries_images', 'value': False,
            'description': 'Save sample inputs, labels, and semantic predictions as images to summary.'},
           {'type': 'string', 'name': 'profile_logdir', 'value': None,
            'description': 'Where the profile files are stored.'},
           {'type': 'float', 'name': 'base_learning_rate', 'value': 0.0001,
            'description': 'The base learning rate for model training.'},
           {'type': 'float', 'name': 'decay_steps', 'value': 0.0,
            'description':
                'Decay steps for polynomial learning rate schedule.'},
           {'type': 'float', 'name': 'end_learning_rate', 'value': 0.0,
            'description': 'End learning rate for polynomial learning rate schedule.'},
           {'type': 'float', 'name': 'learning_rate_decay_factor', 'value': 0.1,
            'description': 'The rate to decay the base learning rate.'},
           {'type': 'integer', 'name': 'learning_rate_decay_step', 'value': 2000,
            'description': 'Decay the base learning rate at a fixed step.'},
           {'type': 'float', 'name': 'learning_power', 'value': 0.9,
            'description': 'The power value used in the poly learning policy.'},
           {'type': 'integer', 'name': 'training_number_of_steps', 'value': 30000,
            'description': 'The number of steps used for training'},
           {'type': 'float', 'name': 'momentum', 'value': 0.9, 'description': 'The momentum value to use'},
           {'type': 'float', 'name': 'adam_learning_rate', 'value': 0.001,
            'description': 'Learning rate for the adam optimizer.'},
           {'type': 'float', 'name': 'adam_epsilon', 'value': 1e-08, 'description': 'Adam optimizer epsilon.'},
           {'type': 'integer', 'name': 'train_batch_size', 'value': 8,
            'description': 'The number of images in each batch during training.'},
           {'type': 'float', 'name': 'weight_decay', 'value': 4e-05,
            'description': 'The value of the weight decay for training.'},
           {'type': 'list', 'name': 'train_crop_size', 'value': '513,513',
            'description': 'Image crop size [height, width] during training.'},
           {'type': 'float', 'name': 'last_layer_gradient_multiplier', 'value': 1.0,
            'description': 'The gradient multiplier for last layers, which is used to boost the gradient '
                           'of last layers if the value > 1.'},
           {'type': 'boolean', 'name': 'upsample_logits', 'value': True,
            'description': 'Upsample logits during training.'},
           {'type': 'float', 'name': 'drop_path_keep_prob', 'value': 1.0,
            'description': 'Probability to keep each path in the NAS cell when training.'},
           {'type': 'string', 'name': 'tf_initial_checkpoint', 'value': None,
            'description': 'The initial checkpoint in tensorflow format.'},
           {'type': 'boolean', 'name': 'initialize_last_layer', 'value': True,
            'description': 'Initialize the last layer.'},
           {'type': 'boolean', 'name': 'last_layers_contain_logits_only', 'value': False,
            'description': 'Only consider logits as last layers or not.'},
           {'type': 'integer', 'name': 'slow_start_step', 'value': 0,
            'description': 'Training model with small learning rate for few steps.'},
           {'type': 'float', 'name': 'slow_start_learning_rate', 'value': 0.0001,
            'description': 'Learning rate employed during slow start.'},
           {'type': 'boolean', 'name': 'fine_tune_batch_norm', 'value': True,
            'description': 'Fine tune the batch norm parameters or not.'},
           {'type': 'float', 'name': 'min_scale_factor', 'value': 0.5,
            'description': 'Mininum scale factor for data augmentation.'},
           {'type': 'float', 'name': 'max_scale_factor', 'value': 2.0,
            'description': 'Maximum scale factor for data augmentation.'},
           {'type': 'float', 'name': 'scale_factor_step_size', 'value': 0.25,
            'description': 'Scale factor step size for data augmentation.'},
           {'type': 'multi_integer', 'name': 'atrous_rates', 'value': None,
            'description': 'Atrous rates for atrous spatial pyramid pooling.'},
           {'type': 'integer', 'name': 'output_stride', 'value': 16,
            'description': 'The ratio of input to output spatial resolution.'},
           {'type': 'integer', 'name': 'hard_example_mining_step', 'value': 0,
            'description': 'The training step in which exact hard example mining kicks off. '
                           'Note we gradually reduce the mining percent to the specified'
                           ' top_k_percent_pixels. For example, if hard_example_mining_step=100K and '
                           'top_k_percent_pixels=0.25, then mining percent will gradually reduce from 100% '
                           'to 25% until 100K steps after which we only mine top 25% pixels.'},
           {'type': 'float', 'name': 'top_k_percent_pixels', 'value': 1.0,
            'description': 'The top k percent pixels (in terms of the loss values) '
                           'used to compute loss during training. '
                           'This is useful for hard pixel mining.'},
           {'type': 'integer', 'name': 'quantize_delay_step', 'value': -1,
            'description': 'Steps to start quantized training. If < 0, will not quantize model.'},
           {'type': 'string', 'name': 'dataset', 'value': 'pascal_voc_seg',
            'description': 'Name of the segmentation dataset.'},
           {'type': 'string', 'name': 'train_split', 'value': 'train',
            'description': 'Which split of the dataset to be used for training'},
           {'type': 'string', 'name': 'dataset_dir', 'value': None, 'description': 'Where the dataset reside.'}]
configs = {entry['name']: entry['value'] for entry in default_configs}

